{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCAfQzWvKYDbOxIbpTeL/7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dedeep007/PROVIS/blob/main/PROVIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "OtoJad1Z2Y7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e030dd-cef4-4e4b-99a1-b84901731c46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "class PromptImageData(Dataset):\n",
        "    def __init__(self, csv_file, root_dir='', transform=None, limit=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        if limit:\n",
        "            self.data = self.data.iloc[:limit].reset_index(drop=True)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.data.iloc[idx]['prompt']\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_file'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'prompt': prompt,   # Raw text\n",
        "            'image': image      # Transformed image\n",
        "        }\n"
      ],
      "metadata": {
        "id": "18E-ctq32ZG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Load dataset (limit to 5000 for fast experimentation)\n",
        "full_dataset = PromptImageData(\n",
        "    csv_file=r\"D:\\PROVIS\\custom_prompts_df.csv\",\n",
        "    root_dir=r\"D:\\PROVIS\",\n",
        "    transform=transform,\n",
        "    limit=10000\n",
        ")\n",
        "\n",
        "# Split into training and validation\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "generator = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "# Print example batch info\n",
        "def print_batch(loader, name=\"\"):\n",
        "    print(f\"\\n{name} Batch Example:\")\n",
        "    batch = next(iter(loader))\n",
        "    print(\"Image tensor shape:\", batch['image'].shape)  # [B, 3, 320, 320]\n",
        "    print(\"Prompt (text) example:\", batch['prompt'][0])  # Just show 1st text prompt\n",
        "\n",
        "print_batch(train_loader, \"Train\")\n",
        "print_batch(val_loader, \"Validation\")\n",
        "\n",
        "print(f\"\\nTotal training examples: {len(train_dataset)}\")\n",
        "print(f\"Total validation examples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "SlbCz5fk2ZN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865cfb6b-4800-4bce-9e90-bde1ca61079c",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Batch Example:\n",
            "Image tensor shape: torch.Size([1, 3, 64, 64])\n",
            "Prompt (text) example: Forrest Gump enjoying a glass of wine by a boat in the style of a modern photograph\n",
            "\n",
            "Validation Batch Example:\n",
            "Image tensor shape: torch.Size([1, 3, 64, 64])\n",
            "Prompt (text) example: anguished sports ball in the style of a wood carving\n",
            "\n",
            "Total training examples: 9000\n",
            "Total validation examples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch from train_loader\n",
        "train_batch = next(iter(train_loader))\n",
        "val_batch = next(iter(val_loader))\n",
        "\n",
        "# Get the shape of one image tensor\n",
        "train_image = train_batch['image'][0]  # Shape: [3, 320, 320]\n",
        "val_image = val_batch['image'][0]      # Shape: [3, 320, 320]\n",
        "\n",
        "# Convert to H x W x C format\n",
        "train_matrix_dim = (train_image.shape[1], train_image.shape[2], train_image.shape[0])\n",
        "val_matrix_dim = (val_image.shape[1], val_image.shape[2], val_image.shape[0])\n",
        "\n",
        "print(\"Train Image Matrix Dimensions (H×W×C):\", train_matrix_dim)\n",
        "print(\"Validation Image Matrix Dimensions (H×W×C):\", val_matrix_dim)\n"
      ],
      "metadata": {
        "id": "cco_UygJ1C8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc92449a-b754-47a6-8b3f-be290a84434e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Image Matrix Dimensions (H×W×C): (64, 64, 3)\n",
            "Validation Image Matrix Dimensions (H×W×C): (64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "class PROVIS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # === Load pretrained components ===\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
        "\n",
        "        # Autoencoder for 64x64 images (downsampled to 8x -> 8x8 latent space)\n",
        "        self.vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(self.device)\n",
        "\n",
        "        # UNet that processes 8x8 latents (64x64 images / 8)\n",
        "        self.unet = UNet2DConditionModel(\n",
        "            sample_size=8,  # 64x64 image -> 8x8 latent\n",
        "            in_channels=4,\n",
        "            out_channels=4,\n",
        "            down_block_types=(\n",
        "                \"DownBlock2D\",\n",
        "                \"CrossAttnDownBlock2D\",\n",
        "                \"CrossAttnDownBlock2D\",\n",
        "                \"DownBlock2D\",\n",
        "            ),\n",
        "            up_block_types=(\n",
        "                \"CrossAttnUpBlock2D\",\n",
        "                \"CrossAttnUpBlock2D\",\n",
        "                \"UpBlock2D\",\n",
        "                \"UpBlock2D\",\n",
        "            ),\n",
        "            block_out_channels=(320, 640, 1280, 1280),\n",
        "            layers_per_block=2,\n",
        "            cross_attention_dim=512,\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "\n",
        "    def get_text_embeddings(self, prompts):\n",
        "        tokens = self.tokenizer(\n",
        "            prompts,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        embeddings = self.text_encoder(**tokens).last_hidden_state\n",
        "        return embeddings, tokens.attention_mask\n",
        "\n",
        "    def forward(self, images, prompts, return_loss=False):\n",
        "        # === Text embeddings ===\n",
        "        text_embeddings, attn_mask = self.get_text_embeddings(prompts)\n",
        "\n",
        "        # === Encode images to latents ===\n",
        "        with torch.no_grad():\n",
        "            latents = self.vae.encode(images).latent_dist.sample()\n",
        "\n",
        "        # === Add noise ===\n",
        "        batch_size = latents.size(0)\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (batch_size,), device=self.device).long()\n",
        "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # === Predict noise ===\n",
        "        noise_pred = self.unet(\n",
        "            sample=noisy_latents,\n",
        "            timestep=timesteps,\n",
        "            encoder_hidden_states=text_embeddings,\n",
        "            encoder_attention_mask=attn_mask\n",
        "        ).sample\n",
        "\n",
        "        if return_loss:\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            return loss\n",
        "\n",
        "        # === Simple denoise ===\n",
        "        denoised_latents = noisy_latents - noise_pred\n",
        "\n",
        "        # === Decode to image ===\n",
        "        with torch.no_grad():\n",
        "            decoded_images = self.vae.decode(denoised_latents).sample\n",
        "\n",
        "        return decoded_images\n"
      ],
      "metadata": {
        "id": "H_eD5KhJ2ZRF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = PROVIS().to(device)\n",
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "BImPLFL42ZUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dd79b4-e76b-4129-8a36-326b76e0b159"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROVIS(\n",
            "  (text_encoder): CLIPTextModel(\n",
            "    (text_model): CLIPTextTransformer(\n",
            "      (embeddings): CLIPTextEmbeddings(\n",
            "        (token_embedding): Embedding(49408, 512)\n",
            "        (position_embedding): Embedding(77, 512)\n",
            "      )\n",
            "      (encoder): CLIPEncoder(\n",
            "        (layers): ModuleList(\n",
            "          (0-11): 12 x CLIPEncoderLayer(\n",
            "            (self_attn): CLIPSdpaAttention(\n",
            "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            )\n",
            "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): CLIPMLP(\n",
            "              (activation_fn): QuickGELUActivation()\n",
            "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            )\n",
            "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (vae): AutoencoderKL(\n",
            "    (encoder): Encoder(\n",
            "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (down_blocks): ModuleList(\n",
            "        (0): DownEncoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): DownEncoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): DownEncoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): DownEncoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (mid_block): UNetMidBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0): Attention(\n",
            "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_out): ModuleList(\n",
            "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "      (conv_act): SiLU()\n",
            "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (decoder): Decoder(\n",
            "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (up_blocks): ModuleList(\n",
            "        (0-1): 2 x UpDecoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-2): 3 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): UpDecoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1-2): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): UpDecoderBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1-2): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (mid_block): UNetMidBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0): Attention(\n",
            "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (to_out): ModuleList(\n",
            "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
            "      (conv_act): SiLU()\n",
            "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (unet): UNet2DConditionModel(\n",
            "    (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (time_proj): Timesteps()\n",
            "    (time_embedding): TimestepEmbedding(\n",
            "      (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
            "      (act): SiLU()\n",
            "      (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "    )\n",
            "    (down_blocks): ModuleList(\n",
            "      (0): DownBlock2D(\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "        (downsamplers): ModuleList(\n",
            "          (0): Downsample2D(\n",
            "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): CrossAttnDownBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0-1): 2 x Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                  (to_k): Linear(in_features=512, out_features=640, bias=False)\n",
            "                  (to_v): Linear(in_features=512, out_features=640, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "        (downsamplers): ModuleList(\n",
            "          (0): Downsample2D(\n",
            "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): CrossAttnDownBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0-1): 2 x Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "        (downsamplers): ModuleList(\n",
            "          (0): Downsample2D(\n",
            "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): DownBlock2D(\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (up_blocks): ModuleList(\n",
            "      (0): CrossAttnUpBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0-2): 3 x Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-2): 3 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (upsamplers): ModuleList(\n",
            "          (0): Upsample2D(\n",
            "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): CrossAttnUpBlock2D(\n",
            "        (attentions): ModuleList(\n",
            "          (0-2): 3 x Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (upsamplers): ModuleList(\n",
            "          (0): Upsample2D(\n",
            "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): UpBlock2D(\n",
            "        (resnets): ModuleList(\n",
            "          (0): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (upsamplers): ModuleList(\n",
            "          (0): Upsample2D(\n",
            "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): UpBlock2D(\n",
            "        (resnets): ModuleList(\n",
            "          (0): ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1-2): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "            (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mid_block): UNetMidBlock2DCrossAttn(\n",
            "      (attentions): ModuleList(\n",
            "        (0): Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=512, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "    (conv_act): SiLU()\n",
            "    (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "\n",
            "Total parameters: 1,063,773,419\n",
            "Trainable parameters: 1,063,773,419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "import lpips\n",
        "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure as SSIM\n",
        "\n",
        "class EffLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 device='cuda',\n",
        "                 mse_weight=0.9,\n",
        "                 vgg_weight=0.89,\n",
        "                 diffusion_weight=0.65,\n",
        "                 lpips_weight=0.8,\n",
        "                 ssim_weight=0.9,\n",
        "                 return_details=True):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.mse_weight = mse_weight\n",
        "        self.vgg_weight = vgg_weight\n",
        "        self.diffusion_weight = diffusion_weight\n",
        "        self.lpips_weight = lpips_weight\n",
        "        self.ssim_weight = ssim_weight\n",
        "        self.return_details = return_details\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.k1_loss_fn = nn.L1Loss()  # for diffusion\n",
        "        self.vgg_loss_fn = nn.L1Loss()\n",
        "\n",
        "        # VGG perceptual loss setup\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features.to(device).eval()\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        return_nodes = {\n",
        "            '4': 'relu1_2',\n",
        "            '9': 'relu2_2',\n",
        "            '16': 'relu3_3',\n",
        "            '23': 'relu4_3'\n",
        "        }\n",
        "        self.vgg_extractor = create_feature_extractor(vgg, return_nodes=return_nodes)\n",
        "\n",
        "        # Normalization for VGG\n",
        "        self.imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "        self.imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "\n",
        "        # Optional LPIPS and SSIM\n",
        "        self.lpips_fn = lpips.LPIPS(net='vgg').to(device).eval()\n",
        "        for param in self.lpips_fn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.ssim_fn = SSIM(data_range=1.0).to(device)\n",
        "\n",
        "    def normalize_for_vgg(self, img):\n",
        "        # Convert from [-1, 1] to [0, 1], then normalize\n",
        "        img = (img + 1.0) / 2.0\n",
        "        return (img - self.imagenet_mean) / self.imagenet_std\n",
        "\n",
        "    def forward(self, generated_images, target_images, noise_pred=None, noise_gt=None):\n",
        "        # ===== MSE Loss =====\n",
        "        mse = self.mse_loss(generated_images, target_images)\n",
        "\n",
        "        # ===== VGG Perceptual Loss =====\n",
        "        vgg_gen = self.normalize_for_vgg(generated_images)\n",
        "        vgg_tgt = self.normalize_for_vgg(target_images)\n",
        "        gen_features = self.vgg_extractor(vgg_gen)\n",
        "        tgt_features = self.vgg_extractor(vgg_tgt)\n",
        "        vgg = sum(self.vgg_loss_fn(gen_features[k], tgt_features[k]) for k in gen_features)\n",
        "\n",
        "        # ===== Diffusion K1 Loss =====\n",
        "        if noise_pred is not None and noise_gt is not None:\n",
        "            diffusion = self.k1_loss_fn(noise_pred, noise_gt)\n",
        "        else:\n",
        "            diffusion = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        # ===== Optional LPIPS and SSIM =====\n",
        "        lpips_score = self.lpips_fn(generated_images, target_images).mean()\n",
        "        ssim_score = 1.0 - self.ssim_fn(generated_images, target_images)  # convert similarity to loss\n",
        "\n",
        "        # ===== Total Loss =====\n",
        "        total_loss = (\n",
        "            self.mse_weight * mse +\n",
        "            self.vgg_weight * vgg +\n",
        "            self.diffusion_weight * diffusion +\n",
        "            self.lpips_weight * lpips_score +\n",
        "            self.ssim_weight * ssim_score\n",
        "        )\n",
        "\n",
        "        if self.return_details:\n",
        "            return total_loss, {\n",
        "                \"total\": total_loss.item(),\n",
        "                \"mse\": mse.item(),\n",
        "                \"vgg\": vgg.item(),\n",
        "                \"diffusion\": diffusion.item(),\n",
        "                \"lpips\": lpips_score.item(),\n",
        "                \"ssim_loss\": ssim_score.item()\n",
        "            }\n",
        "\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "EXyAAg18PDik"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.unet.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, path, device):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model_state = model.unet.state_dict()\n",
        "    ckpt_state = checkpoint['model_state_dict']\n",
        "\n",
        "    filtered_state = {\n",
        "        k: v for k, v in ckpt_state.items()\n",
        "        if k in model_state and v.shape == model_state[k].shape\n",
        "    }\n",
        "\n",
        "    model_state.update(filtered_state)\n",
        "    model.unet.load_state_dict(model_state, strict=False)\n",
        "\n",
        "    try:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    except ValueError as e:\n",
        "        print(\"Warning: Optimizer state_dict could not be loaded.\")\n",
        "        print(e)\n",
        "\n",
        "    try:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    except Exception as e:\n",
        "        print(\"Warning: Scheduler state_dict could not be loaded.\")\n",
        "        print(e)\n",
        "\n",
        "    return checkpoint.get('epoch', 0)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, patience=5, device='cuda',\n",
        "                checkpoint_path=\"Downloads/PROVIS_saved_models/checkpoints/checkpoint.pth\",\n",
        "                best_model_path=\"Downloads/PROVIS_saved_models/best_model.pth\", accumulation_steps=1200):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.AdamW(model.unet.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(\"Loading checkpoint...\")\n",
        "        start_epoch = load_checkpoint(model, optimizer, scheduler, checkpoint_path, device) + 1\n",
        "        print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        for i, batch in progress_bar:\n",
        "            try:\n",
        "                texts = list(batch['prompt'])\n",
        "                images = batch['image'].to(device)\n",
        "\n",
        "                loss = model(images, texts, return_loss=True)\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                loss_value = loss.item() * accumulation_steps\n",
        "                train_losses.append(loss_value)\n",
        "                progress_bar.set_description(f\"Epoch {epoch+1} Training (Loss: {loss_value:.4f})\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e).lower():\n",
        "                    print(\"OOM error at step\", i, \"- skipping batch\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    optimizer.zero_grad()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        val_loss = evaluate(model, val_loader, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={np.mean(train_losses):.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "        save_checkpoint(model, optimizer, scheduler, epoch, checkpoint_path)\n",
        "        print(\"Saved checkpoint.\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            early_stop_counter = 0\n",
        "            torch.save(model.unet.state_dict(), best_model_path)\n",
        "            print(\"Saved model with best loss!\")\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "        torch.cuda.empty_cache()  # Clear memory after each epoch\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            texts = list(batch['prompt'])\n",
        "            images = batch['image'].to(device)\n",
        "\n",
        "            loss = model(images, texts, return_loss=True)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    return np.mean(val_losses)\n"
      ],
      "metadata": {
        "id": "SACqPs5t2ZXg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, num_epochs=50, patience=5, device='cuda')"
      ],
      "metadata": {
        "id": "nQKRT7FVy72_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c72c8e9-e7cf-421e-9a19-f2e3e2fea3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint...\n",
            "Resumed from epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Training (Loss: 1.2557):  11%|████▊                                     | 1020/9000 [48:45<6:22:03,  2.87s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YQi-HzT9Mrhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# === Load model ===\n",
        "model = PROVIS()\n",
        "device = model.device\n",
        "\n",
        "# === Load only UNet weights ===\n",
        "ckpt = torch.load(\"Downloads/PROVIS_saved_models/best_model.pth\", map_location=device)\n",
        "model.unet.load_state_dict(ckpt)\n",
        "model.eval()\n",
        "\n",
        "# === Input prompt ===\n",
        "prompt = [\"ball\"]\n",
        "\n",
        "# === Generate image ===\n",
        "with torch.no_grad():\n",
        "    # Get text embeddings\n",
        "    text_embeddings, attn_mask = model.get_text_embeddings(prompt)\n",
        "\n",
        "    # Start from random latent\n",
        "    latents = torch.randn((1, 4, 8, 8)).to(device)\n",
        "\n",
        "    # DDPM sampling loop\n",
        "    for t in reversed(range(model.scheduler.config.num_train_timesteps)):\n",
        "        timesteps = torch.tensor([t], device=device)\n",
        "        noise_pred = model.unet(\n",
        "            sample=latents,\n",
        "            timestep=timesteps,\n",
        "            encoder_hidden_states=text_embeddings,\n",
        "            encoder_attention_mask=attn_mask,\n",
        "        ).sample\n",
        "\n",
        "        latents = model.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "    # Decode latents to image\n",
        "    decoded = model.vae.decode(latents).sample\n",
        "\n",
        "# === Postprocess and display ===\n",
        "decoded = (decoded.clamp(-1, 1) + 1) / 2  # scale to [0,1]\n",
        "image = to_pil_image(decoded[0].cpu())\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(prompt[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AT18zUOZb8sb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "d6c47d35-a1a5-496d-ce58-5145201a3003"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApAAAAH4CAYAAAD9+t36AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzjElEQVR4nO3deZRdVZk3/u+Z71BTakpIIDMYGQQNKgRCmJowKiggiJKAAi8ggyC90LcVaG18cUAQEQFtRMBGiQOgDIJAE8KoIrQMIRASIGSu+Y5n2L8/8qvbVKqe59aBaFWv/n7Wcrm4T51z9tlnn333PXXrG8sYY0BERERENEr2WDeAiIiIiP5n4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUvm7LSCnT5+OxYsXb9N9PvLII7AsC4888sg23S/9Y412bPz0pz+FZVlYtWrV371N75ZlWfjCF75Q9+fG47n8T7yf9t9/f+y///51f+5/4rlZloVLL730XW87mnE4kr/HXD2SVatWwbIsfOc73/m7H+sf4R/VbyO55ZZbMGfOHHieh5aWljFpw/8242kOH7yXfvrTn45pO/gEchuyLAuWZeG73/3usNrg4PvTn/40Bi3b4pe//CUsy8JvfvObYbXdd98dlmXh4YcfHlabOnUq5s2b949oIhH9D3fPPfe864XwP8qLL76ISy+9dFwsBtJ6+eWXsXjxYsyaNQs33ngjbrjhhrFuEv0v5Y51A9LYb7/9UCqV4Pv+WDdF9e1vfxtnnnkmcrncWDdliH333RcA8Nhjj+GYY46pvd7X14e//e1vcF0Xy5YtwwEHHFCrvfnmm3jzzTdxwgkn/MPbS7S1P/zhD2PdhL+bUqkE1/3HT8nLly+HbW+7Zwn33HMPrr322nG9iHzxxRdx2WWXYf/998f06dPHujmpPPLII0iSBFdffTVmz5491s2hMTBt2jSUSiV4njem7fgf8QSyXC4jSRLYto1MJrNNJ7ttbY899sD69evxox/9aKybMszkyZMxY8YMPPbYY0Nef+KJJ2CMwXHHHTesNvjfg4vPd8sYg1Kp9J72MV6Mh3MpFApjevyx4vv+uP8A+W5lMpkxWUAGQTDmb0Q0ehs2bACAcfur6yiKUK1Wx7oZY+If9d5gWRYymQwcx/m7H0uTeiX2yCOPYM8990Qmk8GsWbNw/fXX49JLL4VlWXW3XblyJY477ji0trYil8thr732wu9///th+7csC7fffjv+5V/+BVOmTEEul0NfX5/4vaY77rgDc+fORTabRXt7Oz7zmc9gzZo1Q35G+u7U4sWLh30Cvf322zF37lw0NjaiqakJu+22G66++upR9c8+++yDAw88EN/61rdGNZBefvllHHvssWhtbUUmk8Gee+6Ju+66q1bv6emB4zj4/ve/X3tt06ZNsG0bbW1tMMbUXj/zzDMxadIk9Xj77rsvnn322SFtW7ZsGXbZZRccdthhePLJJ5EkyZCaZVnYZ599AGyZHL7+9a9j1qxZCIIA06dPx1e+8hVUKpUhx5k+fTqOPPJI3H///dhzzz2RzWZx/fXXi+164YUXcOCBByKbzWL77bfHN77xjSHtkNx1112wLAvPP/987bVf/epXsCwLn/jEJ4b87Pvf/3586lOfqv33TTfdhAMPPBCdnZ0IggA777wzrrvuumHHGM253HbbbXjf+96HTCaDuXPn4tFHH63bdgC49957MX/+fOTzeTQ2NuKII47ACy+8MORnFi9ejIaGBrz22ms4/PDD0djYiJNOOknc5+rVq3HWWWfhfe97H7LZLNra2nDccceN+td1o7mfBtu0Zs0aHH300WhoaEBHRwe+9KUvIY7jIT+7efNmfPazn0VTUxNaWlqwaNEiPPfcc8O+w7Nu3Tqccsop2H777REEAbbbbjt8/OMfH9Luke7jt956C0cffTTy+Tw6OzvxxS9+cdh4HPTUU0/h0EMPRXNzM3K5HBYsWIBly5bV7ZNqtYqvfe1rmDt3Lpqbm5HP5zF//vwRv/Ixkj/96U9YuHAh2tvbkc1mMWPGDJx66qlDfmbr70AOzquvvvoqFi9ejJaWFjQ3N+OUU05BsVise8xvfOMbsG0b11xzjfpzW3+Xb/DrNsuWLcMFF1yAjo4O5PN5HHPMMdi4caO6r8WLF+Paa6+tnc/g/7Z2ww031OaQD3/4w3jmmWeG/Uy9uVGjzeE//elPcdxxxwEADjjggFobH3nkESxatAjt7e0Iw3DYPg855BC8733vU4/b09OD888/HzvssAOCIMDs2bNxxRVXjGouA4Af/vCH2GWXXRAEASZPnoyzzz4bPT09tfr06dNxySWXAAA6Ojrqfm/2+eefx+LFizFz5kxkMhlMmjQJp556KjZv3jyq9pTLZVx66aXYaaedkMlksN122+ETn/gEXnvtNQBDv9d61VVX1a7piy++OKp7xhiD6dOn4+Mf//iIx25ubsYZZ5xRe23Dhg343Oc+h4kTJyKTyWD33XfHzTffPGS7d7ZpNONsJKN9P9LeG0bz/nLBBRcMex8/55xzYFnWkPf89evXw7Ks2vYjfQfy7zEn15Pq4+6zzz6LQw89FNtttx0uu+wyxHGMf/3Xf0VHR0fdbdevX4958+ahWCzi3HPPRVtbG26++WZ87GMfw5IlS4b8ShUAvv71r8P3fXzpS19CpVIRnzr89Kc/xSmnnIIPf/jD+OY3v4n169fj6quvxrJly/Dss8+m/pT2wAMP4MQTT8RBBx2EK664AgDw0ksvYdmyZTjvvPNGtY9LL70U++23H6677jpccMEF4s+98MIL2GeffTBlyhRcfPHFyOfz+OUvf4mjjz4av/rVr3DMMcegpaUFu+66Kx599FGce+65ALY8FbQsC11dXXjxxRexyy67AACWLl2K+fPnq23bd999ccstt+Cpp56qvREvW7YM8+bNw7x589Db24u//e1v+MAHPlCrzZkzB21tbQCAz3/+87j55ptx7LHH4sILL8RTTz2Fb37zm3jppZeGfbdy+fLlOPHEE3HGGWfgtNNOEyffdevW4YADDkAURbV+uOGGG5DNZuv29b777gvLsvDoo4/W2rx06VLYtj3kaerGjRvx8ssvD/lDg+uuuw677LILPvaxj8F1Xdx9990466yzkCQJzj777FGfy3/+53/iF7/4Bc4991wEQYAf/vCHOPTQQ/H0009j1113Fdt+yy23YNGiRVi4cCGuuOIKFItFXHfddbVF/js/2ERRhIULF2LffffFd77zHfXrEc888wwef/xxnHDCCdh+++2xatUqXHfdddh///3x4osvqtumuZ/iOMbChQvx0Y9+FN/5znfw4IMP4rvf/S5mzZqFM888EwCQJAmOOuooPP300zjzzDMxZ84c3HnnnVi0aNGwY3/yk5/ECy+8gHPOOQfTp0/Hhg0b8MADD+CNN94Qf81YKpVw0EEH4Y033sC5556LyZMn45ZbbsFDDz007GcfeughHHbYYZg7dy4uueQS2LZdm+SXLl2Kj3zkI2K/9PX14cc//jFOPPFEnHbaaejv78dPfvITLFy4EE8//TT22GMPcdsNGzbgkEMOQUdHBy6++GK0tLRg1apV+PWvfy1u807HH388ZsyYgW9+85v4y1/+gh//+Mfo7OyszU8j+Zd/+RdcfvnluP7663HaaaeN6jhbO+ecczBhwgRccsklWLVqFa666ip84QtfwC9+8QtxmzPOOANvv/02HnjgAdxyyy0j/szPf/5z9Pf344wzzoBlWfjWt76FT3ziE1i5cmXtSeho5kZJvTl8v/32w7nnnovvf//7+MpXvoL3v//9ALZ8wPzsZz+Ln/3sZ7j//vtx5JFH1va5bt06PPTQQ7XF20iKxSIWLFiANWvW4IwzzsDUqVPx+OOP48tf/jLWrl2Lq666StwW2PK+cdlll+Hggw/GmWeeieXLl+O6667DM888g2XLlsHzPFx11VX42c9+ht/85je47rrr0NDQUJv3pL5YuXIlTjnlFEyaNAkvvPACbrjhBrzwwgt48skn1Qc/cRzjyCOPxB//+EeccMIJOO+889Df348HHngAf/vb3zBr1qzaz950000ol8s4/fTTEQQBWltbR3XPWJaFz3zmM/jWt76Frq4utLa21vZ59913o6+vD5/5zGcAbLnX999/f7z66qv4whe+gBkzZuCOO+7A4sWL0dPTM+z9eTTjbCRp34+k94bRvL/Mnz8f3/ve9/DCCy/U3isG37+WLl1ae89funQpgC1f49Ns6zm5LpPCUUcdZXK5nFmzZk3ttRUrVhjXdc3Wu5o2bZpZtGhR7b/PP/98A8AsXbq09lp/f7+ZMWOGmT59uonj2BhjzMMPP2wAmJkzZ5pisThkn4O1hx9+2BhjTLVaNZ2dnWbXXXc1pVKp9nO/+93vDADzta99rfbaggULzIIFC4ad06JFi8y0adNq/33eeeeZpqYmE0XRqPtlEABz9tlnG2OMOeCAA8ykSZNq53DTTTcZAOaZZ56p/fxBBx1kdtttN1Mul2uvJUli5s2bZ3bcccfaa2effbaZOHFi7b8vuOACs99++5nOzk5z3XXXGWOM2bx5s7Esy1x99dVqG1944QUDwHz96183xhgThqHJ5/Pm5ptvNsYYM3HiRHPttdcaY4zp6+szjuOY0047zRhjzF//+lcDwHz+858fss8vfelLBoB56KGHaq9NmzbNADD33XffsDZIY+Opp56qvbZhwwbT3NxsAJjXX39dPadddtnFHH/88bX//tCHPmSOO+44A8C89NJLxhhjfv3rXxsA5rnnnqv93NbjyxhjFi5caGbOnDmsvdK5ADAAzJ/+9Kfaa6tXrzaZTMYcc8wxtdcGr//gufT395uWlpZa3w5at26daW5uHvL6okWLDABz8cUXq/2gndcTTzxhAJif/exntdfey/002KZ//dd/HXKcD37wg2bu3Lm1//7Vr35lAJirrrqq9locx+bAAw80AMxNN91kjDGmu7vbADDf/va31XPb+j6+6qqrDADzy1/+svZaoVAws2fPHnJuSZKYHXfc0SxcuNAkSTKkr2bMmGH+6Z/+ST1uFEWmUqkMea27u9tMnDjRnHrqqeq2v/nNb4bd+yMBYC655JLaf19yySUGwLD9H3PMMaatrW3YtoNzz4UXXmhs2zY//elP1eMN2vp+HByrBx988JC++uIXv2gcxzE9PT3q/s4+++xh7wfGGPP6668bAKatrc10dXXVXr/zzjsNAHP33XfXXhvt3DiS0czhd9xxx5DxMSiOY7P99tubT33qU0Nev/LKK41lWWblypW117but69//esmn8+bV155Zci2F198sXEcx7zxxhtiezZs2GB83zeHHHJI7b3QGGN+8IMfGADm3//932uvDY6LjRs3ivsbNNJc8B//8R8GgHn00UfVbf/93//dADBXXnnlsNrguBi8pk1NTWbDhg1Dfma098zy5csNgNp72aCPfexjZvr06bVjDd7rt956a+1nqtWq2XvvvU1DQ4Pp6+sb0qbRjLORpHk/0t4bRvP+smHDBgPA/PCHPzTGGNPT02Ns2zbHHXfckPf8c88917S2tg7r98H505htPyePxqh/hR3HMR588EEcffTRmDx5cu312bNn47DDDqu7/T333IOPfOQjQ75L19DQgNNPPx2rVq3Ciy++OOTnFy1aVPcJ1J/+9Cds2LABZ511FjKZTO31I444AnPmzBn26/HRaGlpQaFQwAMPPJB623e69NJLsW7dOvG7kF1dXXjooYdw/PHHo7+/H5s2bcKmTZuwefNmLFy4ECtWrKj92nD+/PlYv349li9fDmDLp5H99tsP8+fPr30yeeyxx2CMqfsE8v3vfz/a2tpqT+eee+45FAqF2l9Zz5s3r/YrvSeeeAJxHNeu2T333AMAw56qXnjhhQAwrL9nzJiBhQsX1u2re+65B3vttdeQJ0AdHR3qr2nf6Z390N/fj+eeew6nn3462tvba68vXbq09jR30DvHV29vLzZt2oQFCxZg5cqV6O3tHfW57L333pg7d27tv6dOnYqPf/zjuP/++4f96mDQAw88gJ6eHpx44om1a79p0yY4joOPfvSjI/5qdPATZD3vPK8wDLF582bMnj0bLS0t+Mtf/iJu927up//zf/7PkP+eP38+Vq5cWfvv++67D57nDXkKZtv2sCe82WwWvu/jkUceQXd396jOE9gydrbbbjsce+yxtddyuRxOP/30IT/317/+FStWrMCnP/1pbN68udbfhUIBBx10EB599FH114yO49R+C5IkCbq6uhBFEfbcc0+1T4H//q7a7373uxF/NVrPSH28efNm9PX1DXndGIMvfOELuPrqq3Hrrbe+uycK73D66acPeUI1f/58xHGM1atXv6f9fupTn8KECROG7BdAbdykmRtH8l7mcNu2cdJJJ+Guu+5Cf39/7fXbbrsN8+bNw4wZM8Rt77jjDsyfPx8TJkwYck8ffPDBiONY/VrLgw8+iGq1ivPPP3/Id/xPO+00NDU1vav3MmDoXFAul7Fp0ybstddeAFB33P7qV79Ce3s7zjnnnGG1rZ9cfvKTnxz2m8jR3jM77bQTPvrRj+K2226rvdbV1YV7770XJ510Uu1Y99xzDyZNmoQTTzyx9nOe5+Hcc8/FwMAA/vM//3PI8euNM0na9yPpvWE07y8dHR2YM2dObWwsW7YMjuPgoosuwvr167FixQoAW96/Bn/bVs+2mpNHY9QLyA0bNqBUKo34V1+j+Uuw1atXj/grzMFfH2w9KWk36jv3CWDE/c6ZM+ddTXRnnXUWdtppJxx22GHYfvvtceqpp+K+++5LvZ/99tsPBxxwgPhdyFdffRXGGHz1q19FR0fHkP8N/ppk8MvSgwN/6dKlKBQKePbZZzF//nzst99+QxZITU1N2H333dV2WZaFefPm1b7ruGzZMnR2dtau4TsXkIP/P7iAXL16NWzbHna9J02ahJaWlnd1DQf3u+OOOw57vd73jQbNnz8fa9euxauvvorHH38clmVh7733HrKwXLp0KfbZZ58hk/OyZctw8MEHI5/Po6WlBR0dHfjKV74CACMuICUjtX2nnXZCsVgUvzM2ODEceOCBw67/H/7wh9q1H+S6LrbffvtR9MaWX/V87Wtfq30Pq729HR0dHejp6Rl2Xu+U9n7KZDLD3jQmTJgwZAG4evVqbLfddsN+bb71GAqCAFdccQXuvfdeTJw4Efvttx++9a1vYd26deq5rl69GrNnzx42sW59DoP9vWjRomH9/eMf/xiVSkXtGwC4+eab8YEPfACZTAZtbW3o6OjA73//+7rbLViwAJ/85Cdx2WWXob29HR//+Mdx0003id/T3NrUqVOH/Pfgm+LWC+2f/exnuPbaa3HNNdcMeZN9t0Z73G293zRz40je6xx+8skno1Qq1b6Ss3z5cvz5z3/GZz/7WXW7FStW4L777hvW5oMPPrhum6V7z/d9zJw5810v2ru6unDeeedh4sSJyGaz6OjoqM1l9cbta6+9hve9732j+sMuaX4c7T1z8sknY9myZbXzvOOOOxCG4ZA+H3yf2PqPaKU1xLsdv2nfj6RzH+37y9bvU3vuuSf23HNPtLa2YunSpejr68Nzzz1X9+EQsG3n5NEYtzE+o/n+WxqWZQ35ouqgrZ8QdXZ24q9//Svuv/9+3Hvvvbj33ntx00034eSTTx72Zd16LrnkEuy///64/vrrh30Xc/Bpx5e+9CXxydbgBR386+lHH30U06dPhzEGe++9Nzo6OnDeeedh9erVWLp0KebNmzeqv1Dfd999cffdd+O//uu/at9/HDRv3jxcdNFFWLNmDR577DFMnjwZM2fOHLL9aD4FAdv+GkoGF7iPPvooVq5ciQ996EO1L2x///vfx8DAAJ599ln827/9W22b1157DQcddBDmzJmDK6+8EjvssAN838c999yD733ve8OeRm3rcxnc/y233DLiHz5tPWkHQTDq9IFzzjkHN910E84//3zsvffeaG5uhmVZOOGEE0b9Zf7R2NZ/AXj++efjqKOOwm9/+1vcf//9+OpXv4pvfvObeOihh/DBD37wPe178Ly//e1vi99XbGhoELe/9dZbsXjxYhx99NG46KKL0NnZCcdx8M1vfrP2BwUSy7KwZMkSPPnkk7j77rtx//3349RTT8V3v/tdPPnkk+pxAbmft57P9tlnH/z1r3/FD37wAxx//PFDvk/2boz2uNt6v2nmxpG81zl85513xty5c3Hrrbfi5JNPxq233grf93H88cer2yVJgn/6p3/CP//zP49Y32mnneoee1s7/vjj8fjjj+Oiiy7CHnvsgYaGBiRJgkMPPXSbzgUjzY9p7pkTTjgBX/ziF3HbbbfhK1/5Cm699Vbsueeeo36IMJK/1/jd2kjnnub9Zd9998WNN96IlStX1v6OwbIs7Lvvvli6dCkmT56MJElGtYD8R/9V9qgXkJ2dnchkMnj11VeH1UZ6bWvTpk2r/Qr2nV5++eVaPa3BbZYvX44DDzxwSG358uVD9jlhwoQRH12P9MnO930cddRROOqoo5AkCc466yxcf/31+OpXv5pqlb5gwQLsv//+uOKKK/C1r31tSG1wUeZ5Xu0Tqmb+/Pl49NFHMWPGDOyxxx5obGzE7rvvjubmZtx33334y1/+gssuu2xU7XpnHuSyZctw/vnn12pz585FEAR45JFH8NRTT+Hwww+v1aZNm4YkSbBixYrapz5gyx9I9fT0vKtrOLjfwSdE7zTSeBnJ1KlTMXXqVCxduhQrV66s3Wj77bcfLrjgAtxxxx2I43jIF5DvvvtuVCoV3HXXXUM+qY72r2rfaaS2v/LKK8jlcuIfmA1+Ab2zs3NU1z+NJUuWYNGiRUMC7cvl8pC/5hxJmvtptKZNm4aHH34YxWJxyCdeac6YNWsWLrzwQlx44YVYsWIF9thjD3z3u9/FrbfeKu7/b3/7G4wxQz7YbD12Bvu7qanpXfX3kiVLMHPmTPz6178echztjyq2ttdee2GvvfbCv/3bv+HnP/85TjrpJNx+++34/Oc/n7o9I5k9eza+9a1vYf/998ehhx6KP/7xj2hsbNwm+05jtB8wJWnnxpHUm8PrtfHkk0/GBRdcgLVr1+LnP/85jjjiiCG/Dh3JrFmzMDAw8K7a/M57750f2KvVKl5//fV3tc/u7m788Y9/xGWXXTbk/Wek+Woks2bNwlNPPYUwDN9VzFOae6a1tRVHHHEEbrvtNpx00klYtmzZsD86mjZtGp5//vlapN+g97KGGMl7fT8C0r2/DL5fPfDAA3jmmWdw8cUXA0DtD3EnT56MfD4/5GtS70XaOVkz6l9hO46Dgw8+GL/97W/x9ttvDznovffeW3f7ww8/HE8//TSeeOKJ2muFQgE33HADpk+fjp133jll04E999wTnZ2d+NGPfjTk10H33nsvXnrpJRxxxBG112bNmoWXX355yK8Un3vuuWERHlvHG9i2Xfsrt9H+yumdBr8LufW/FtDZ2Vl7Orl27dph2239q8/58+dj1apV+MUvflEbcLZtY968ebjyyisRhuGoPqEAqMUw3XbbbVizZs2QJ5BBEOBDH/oQrr32WhQKhSHfWR1cTG59Y1955ZUAMKS/0zj88MPx5JNP4umnn669tnHjxiHfialn/vz5eOihh/D000/X+mFwof3//t//QzabHXIDDn5Se+en0d7eXtx0002p2//EE08M+U7Pm2++iTvvvBOHHHKI+Ilw4cKFaGpqwuWXXz7i9+LqxaVoHMcZ9in7mmuuEb+POSjN/TRaCxcuRBiGuPHGG2uvJUlSi3oZVCwWUS6Xh7w2a9YsNDY2qvfd4YcfjrfffhtLliwZsq+t77e5c+di1qxZ+M53voOBgYFh+6nX3yONl6eeemrIfCbp7u4edj0Gn4K+mzlF84EPfAD33HMPXnrpJRx11FFjkleaz+cBoO4HFknauXFro5nD67XxxBNPhGVZOO+887By5craXwJrjj/+eDzxxBO4//77h9V6enoQRZG47cEHHwzf9/H9739/yFj5yU9+gt7e3nd17400ZoHh87fkk5/8JDZt2oQf/OAHw2qjeYqX9p757Gc/ixdffBEXXXQRHMcZ9o9XHH744Vi3bt2QFIAoinDNNdegoaEBCxYsGNV51bMt3o/SvL/MmDEDU6ZMwfe+9z2EYViLzJs/fz5ee+01LFmyBHvttdc2y4gd7Zw8GqladOmll+IPf/gD9tlnH5x55pmI4xg/+MEPsOuuu+Kvf/2ruu3FF1+M//iP/8Bhhx2Gc889F62trbj55pvx+uuv41e/+tW7Cgf3PA9XXHEFTjnlFCxYsAAnnnhiLXZk+vTp+OIXv1j72VNPPRVXXnklFi5ciM997nPYsGEDfvSjH2GXXXYZ8mX0z3/+8+jq6sKBBx6I7bffHqtXr8Y111yDPfbYY8hTt9FasGABFixYMOwLvgBw7bXXYt9998Vuu+2G0047DTNnzsT69evxxBNP4K233sJzzz1X+9nBRdHy5ctx+eWX117fb7/9cO+999ZyrkbD9318+MMfxtKlSxEEwbBPNvPmzas9vXrnAnL33XfHokWLcMMNN6CnpwcLFizA008/jZtvvhlHH330kH/BJo1//ud/xi233IJDDz0U5513Xi02YfAT52jMnz8ft912W+3RP7DlJp43bx7uv/9+7L///kOioA455JDaU4ozzjgDAwMDuPHGG9HZ2Tnim5Zm1113xcKFC4fE+ABQnwg3NTXhuuuuw2c/+1l86EMfwgknnICOjg688cYb+P3vf4999tlnxIl7NI488kjccsstaG5uxs4774wnnngCDz74YC2KSZLmfhqto48+Gh/5yEdw4YUX4tVXX8WcOXNw1113oaurC8B/P6165ZVXcNBBB+H444/HzjvvDNd18Zvf/Abr169X/xWk0047DT/4wQ9w8skn489//jO222473HLLLcO+32PbNn784x/jsMMOwy677IJTTjkFU6ZMwZo1a/Dwww+jqakJd999t3icI488Er/+9a9xzDHH4IgjjsDrr7+OH/3oR9h5551HXJC+080334wf/vCHOOaYYzBr1iz09/fjxhtvRFNT05An/NvKXnvthTvvvBOHH344jj32WPz2t7/9hwaFD84n5557LhYuXDjiYqCeNHPj1kYzh++xxx5wHAdXXHEFent7EQRBLbMP2PLHDYceeijuuOMOtLS0jGoBd9FFF+Guu+7CkUceicWLF2Pu3LkoFAr4r//6LyxZsgSrVq1Ce3v7iNt2dHTgy1/+Mi677DIceuih+NjHPobly5fjhz/8IT784Q+PagG7taamptp3icMwxJQpU/CHP/wBr7/++qi2P/nkk/Gzn/0MF1xwQe2DeaFQwIMPPoizzjprxOzGd0p7zxxxxBFoa2vDHXfcgcMOO6x2LQadfvrpuP7667F48WL8+c9/xvTp07FkyZLa08pt9bR9W7wfpX1/mT9/Pm6//XbstttutSfdg1/FeuWVV/DpT396m5wbMPo5eVRG/ffa/78//vGP5oMf/KDxfd/MmjXL/PjHPzYXXnihyWQyQ35u64gDY4x57bXXzLHHHmtaWlpMJpMxH/nIR8zvfve7IT8zGC1yxx13DDv21rEjg37xi1+YD37wgyYIAtPa2mpOOukk89Zbbw3b/tZbbzUzZ840vu+bPfbYw9x///3DYnyWLFliDjnkENPZ2Wl83zdTp041Z5xxhlm7dm3dvsE7ojRGajdGiPJ47bXXzMknn2wmTZpkPM8zU6ZMMUceeaRZsmTJsP10dnYaAGb9+vW11x577DEDwMyfP79u+97py1/+sgFg5s2bN6w2GHnT2Ng4LAojDENz2WWXmRkzZhjP88wOO+xgvvzlLw+J2zBmy/U/4ogjRjz2SGPj+eefNwsWLDCZTMZMmTLFfP3rXzc/+clPRhXjY8x/xxO9//3vH/L6N77xDQPAfPWrXx22zV133WU+8IEPmEwmY6ZPn26uuOKKWnTF1lEN0rkMXvNbb73V7LjjjiYIAvPBD35w2BjdOsZn0MMPP2wWLlxompubTSaTMbNmzTKLFy8eEgu0aNEik8/n6/bBoO7ubnPKKaeY9vZ209DQYBYuXGhefvnlYf3+Xu4nqU2DESPvtHHjRvPpT3/aNDY2mubmZrN48WKzbNkyA8DcfvvtxhhjNm3aZM4++2wzZ84ck8/nTXNzs/noRz86JJ7HmJHjuFavXm0+9rGPmVwuZ9rb2815551n7rvvvhHP7dlnnzWf+MQnTFtbmwmCwEybNs0cf/zx5o9//KPap0mSmMsvv9xMmzatdo1/97vfDZs/RvKXv/zFnHjiiWbq1KkmCALT2dlpjjzyyCHX2Bg5xmfruJaRxtJIc8+dd95pXNc1n/rUp4ZEw2xNivHZeq6SxsvWoigy55xzjuno6DCWZdXGw2D0yEhRTVufuzHp5sZ3Gu0cfuONN5qZM2cax3FGPK9f/vKXBoA5/fTTRzzOSPNYf3+/+fKXv2xmz55tfN837e3tZt68eeY73/mOqVararuN2RLbM2fOHON5npk4caI588wzTXd395CfSRPj89Zbb5ljjjnGtLS0mObmZnPccceZt99+e8T+HkmxWDT/9//+39p8P2nSJHPsscea1157zRijX9N3c8+cddZZBoD5+c9/PmJ9/fr1tbnN932z2267DYudSTvORjLa9yPtvWG07y/GGHPttdcaAObMM88c8vrBBx9sAAybn6QYn205J4+GZcx7/0bp0UcfjRdeeGHU360gov/dfvvb3+KYY47BY489VvuVDdF4cuedd+Loo4/Go48+OuqvB9F788UvfhE/+clPsG7dOvUfPKBt793Myal/b7z1d2pWrFiBe+65Z8R/JpCIaOs5I45jXHPNNWhqasKHPvShMWoVke7GG2/EzJkzh3yNh/5+yuUybr31Vnzyk5/k4vHvbFvNyam/lTlz5szav625evVqXHfddfB9X4wuIKL/3c455xyUSiXsvffeqFQq+PWvf43HH38cl19++T8s6olotG6//XY8//zz+P3vf4+rr776Pf9VOek2bNiABx98EEuWLMHmzZtH/U8G07u3rebk1L/CPuWUU/Dwww9j3bp1CIIAe++9Ny6//HI+SSCiEf385z/Hd7/7Xbz66qsol8uYPXs2zjzzzCH/LjnReGFZFhoaGvCpT30KP/rRj7bZX7/SyB555BEccMAB6OzsxFe/+lXOC/8A22pO3ibfgSQiIiKi/z3SZ+cQERER0f9qXEASERERUSpcQBIRERFRKvx28Dhw4ec+IdaaJsjp+jYy6n6L5aJYy2Xlbft6+sVa20T9XzMZ6CuINdeT/6H3kf45v0EW5L+CdP1AbU+pv0esNba0iLX+otwHTU3yduWSfB4AYCB/5dh25H8tRPtnCH1f/1dGkkSryft1HHl6qEbyeQaefk2iWOmjRP5Ma9ly30n/ZOR/byyXysWyWAve8a8Xba0Syf8MYVKtqs3xM3Ifxco1iSryP4eXb9D/erJYkdtkG7n/tL9C1v5Aud7X6z3l3k2UQWuM3D+uq4wDZTsAiOWuha3sN1Y29Bz9GY3Wf4kyV2jzCJTTdHy9PVq1UJL/xSXHlucg35PvIQCwlT7IKiuUC79xo1ykfwg+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMONDTkxFqloGSwQI4fAepEYbhylIqfkSN+woIcXQIAtjKkSiW5vR7kmIzEkj/nNGab1fasW7tZrPmB3AdZv0WsbVjbLbdHifgBgBjyNfFcJcZHGQZhRRsjgOXK/WeMEpuj9LvWnjjU41ISW95voEQZlZXYHCdRskAAFMOSWMsFynhX4qWUSwnf1yN1SgPyuQRZuQ8yvty361ZtVI/ZMX07sRZW5fN0bfmejpQIG1vLqAEALdJKyV0KY3k7bc5LjH6fOJ7c746r5e3I4zms6veCFhOVJHLfWkqKj5XIxbist8cJ5D7IWQ1iTUnxQWT0cRAq8UmOci409vgEkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUmGMzzjgZwOxVi5VxZqX0df/WqROsaLEb9hKhIYS+wLoMTXafh0lMsZy5O02bX5bbU9nW4t8zEDun0SJb7F9ObPCVc4RAIwtx2gkiXytbU/uH7tOTIaSloLYlothRW6P78uxS/UidfpLcqSOl5e3TRy57+JYP6atxLBEUPZrKbEnSt95Gbl/ACAxSu5JIh8ztOQx2z65XT0mlBgkT+kfo/SBFiej9Q8AwNHieOT5yfXk7SxlsCdVPcbHVdprlFNJHHm/2rwGAJEln6ej3PNGmZ8c5d7U4pEAIAzlc4kieT7I+spSok4fuErMWCWUj0ljj08giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgV5kCOA4XesljLZDJizaqTOVguyhlaQS4r1qpluT11YiCR8X15WyVSb31vl1hrb24Ra9VQz5rzlS4yShafm5Wz1Nozct/19PSp7WnI5cVaKZTzEbOBPA5MnZw1GCWnTvkMaXny9BAqmXD9yvgBgIaMnHvqO/I10eIlPbtOJqoycKtVOVPPteRxkDhyrVCUMxcBICzJ9axyDzlKZp7r6H3gBHK/d2/uFmsNjfKYtZU+MEqeJQCYqpyBGLjyOKjE8tiLlajH2MjHAwAkcr/HoXwujpJTa9tK3icAo7RJm95DpeYqoZWZbE5tz+qVL4u1nXaYJdZKkZLbqYxZALCNPIY8ZRzQ2OMTSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoUxPuOAq0SX+Bn5ElWVaAkA8HNyZEO1KseI2I58TK2tAFAoylE0jhIz0jGhUaxFsRwRocUcAUAhLIo1ryT3X1iUa4ESh6JF1ACAo2TROErkR3dfj1hrbWpRjxkp0SaOq00B8oaWss9sTr8miZLnFEOLxhkQa7k646Bakce7p8QVmUSORLGVeKScEvUEAAOh8tndkseIgdyesnKfAIBblrfNKXOFpQwgo8T4RJE+P2npU2VlW0eJbNJSX3IZOY4IALTp1HfkiJ+SEgPlKfcQALjKXFsoyXNXXhnvRjlm34A8PwPAzOnTxVqpLMcnJUocUQb6nBjrSWw0jvEJJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIYn3HACeTsiUpVjk5wrDpxKbacS+EG8raOLecqVGK5PQBgK1E9ga1EEikRI4ktR0RYWp4MAEuJhUmU2JOsK8d2xErEiHHl4wFAOSrI7VFOpb25VW5PnagQT2mSdj0dJWfFVvZZjcpqe1qbJoi1NZu65O3y8pgthvoxM658LpVQG1/yPrWx3tXTo7Yn8OV73lFihWxbjkSJY70PlNsa2rMEy5M7IU7k8ZPx9OcTFaU9USJfE0e5py0lx0dJXQIA2Lb8A4klN9ZT+kcbIwBgKfv1fSUiKVS2U+5NTysCiKvyfrVxqUWtVav14pyUsVdnbqOxxSeQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKF5BERERElAoXkERERESUCmN8xoEokqMTYqNEaFihut+cEtmQKPkkUVmO5vBcJdcEQCWW29QXyxE2E7ItYm2gIh8vivWIiGw2K9bCqrzjWOkfV4nqiZRYEwAwUPo9kdtj2w3KdnrURbkiR2y4lhx7knjyfh0lkqnOEEGhXBJrzY1yVE+5Io8t35PPAwAqSvRSVoknCW3lekVye7LZOhFbidyeBPJ8YIwS1VMnMqZSUSJ3lLnCMnL/WMp4Vk5jy7ZKhI2rRLvYjnZN5LGOWG9QokTGuErUmu0o0TjK/A0AiVLuHyiKtZwWw+bI10sZdgD0a5KEynygHNMo8UiAfq2TOv1HY4tPIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFeZAjgOZwBdrVSW7rVonaK1UkLfVMs98JZdr40Y5yxEApk1rE2tvvi23x22UzyXql7drbmtR21Msy1lqrtLvMEp2WSxnzUVKViEABFk5z9H25Dy5UihnJwaenjkYK8GMhaKcPZnPaPuVr5dllFw8AJWqltUn12xXyZozesBdoGR3hko+YKy01THydpmMMrYAlKvyOEliJYPUk4+pxPQBAHxbzjJ0Lfme1/ZrlEtZhH5NMr7SR0r8XxgpeY1aRmSdvNRMNhBrhfK7u09MneDFSDmXfCBn2Pq+fC2VCFtY2rwGfUFgXHmMVLU8XkufDyJlLnGVjFYae3wCSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKlwAUlEREREqfBv5MeBrq4usZZVIloyGf3yVWw5HqFSlGNhvMZGsdY6QY6WAIBipSzW8kqMyKo3Nou1SZPaxdr6DZvU9kyZNEms9fb3ibVqKMesBFocSlauAYBty3EXjnI72kqcxbq1a9VjNjY1ibVAG0OJFoki57e4jh7b4QVyXEollI9p20rN0uNJqqEcBZXPyfdYX798noESA1VU7i8A8DwlwkaJojGREt2lxAoBgOPK/V41cnszjtzWULnWVqhk/ACIIvleKCmxOU2NchSWdt+6vj5fliJ5jDT4ct/FSlyRfkUA15b7L1ae72ij3VZiqWxPn58M5GtmKdlKrhKjZZT5EgBsRz5PSz1TGmt8AklEREREqXABSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKkwxmccaGiSYylsJQKhu7tH3a9jyZEM2bwcXVIsFcRaxtUjWoIgL9ZKrhyx0d7eLNZsJepi+vSZanteeeUVsTZlh+3EmleVIz185ZpUlTgLAPCUSJSugV6x1ha0yLX2NvWYUSRHYTiOfC6WkkESx/J5dm3sUdszeYrc77YjT0lxpSjvVIvFAWAr51mqyPeJ48mfsQeKyn2inAcAGKX/HCWuKNMg3yf/tfoJ9Zi7TZ4r1gZC+TwjZSB4trxdNdRDbDwlvsV15eulbAYv0NqjxwoFytxWUmKgbC22KtHng5IS9NOWk98XBpT2KClQcJTrBQCx0kVadJejREgFvh7jEyl9YEI5Fo7GHp9AEhEREVEqXEASERERUSpcQBIRERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKozxGQcsR45ZCUM5KqQhp1++uCLXS0U5UiefycrtMXJ8BAB09wzIRUvOl2hoaBRrA8U+sVbeJJ8HAGw/sV2s5S05yujVrg1ibcqkDrHmJvpnskoot7cxL8d2lJQ4C9fXI2yMK0eJ5DKBWOvv7le2k8dIZkqT2p7NG9eKNS+XE2uBEudULy7FV2J+BvrlMWspkTo5T+67KNEjbGzI+42UKKiBXvlemLODHNMDAMViSaxlfHmuqCrnUi3L+3RcfX6KIJ9nsxJt1l+Ur1egHNNTooEAoDAg7zeXlePJEmXs1TtmokRsFcKKWNMihxxbicVR4qMAQBmWcJR8oIo23pXzAPSousTiM67xjFeHiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhSYYzPOLBudbdYa21uFmtWTslcAFCO5IgNz5MvfWzJUQ+lYqQes2VCi1iLynIUDZRjGiW+xarzGaisxOYYT46X6GyfINaishy9kVh6fIvnynEy5UiOSMookTGJEvsCAImSdNRX6ZW3U4ZXqLTVVyJGACDfLEc2+ZAjPTYrETb5rBzJBAClsrxtJpAjifJ5OVZozdtvirW2djk+CgCqVfk+si15fDlK10ZlPWLLV/oorMjb2spBHWUeMbZ+bzpGvlfKylzhK9E4Rhvstv52l8nL4wBGvibaXsM6sTmOI/eRpcSewcjzSDWU533L6PemckngKDXXUWKglDkYAAJlDFnKfmns8QkkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCkKVxoLNTzhzMTWgQa4UBOe8LANrb5Lw9AzkPLFZiu7xWPW8viuV8O8+X89uKBflccg1yPltFya8DAMeT89KgZfEpH60sT85nc+vcUrGS2ehY8jWJQiVPTsmSA4AkUfJAlUy9KJGPaXnyMYuVotoe15OPWajK+X+NjfJ2vq/n21kledwmiZzx11foF2t5X743S/16H1jKAKuU5W3zefmYfqDnwlrKedrKppatBAAmcr+Xq/q96bryvRIp+YmBLY+DxJFrUM4fACyj3EfK8OruLoi1xrw+XxotJ1LJgXRcORe2WpHnNVe70AA8T7uPlLzGWMmwzchtBYAoUsZlnRxNGlt8AklEREREqXABSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKkwxmccqIZyTIZbqog131IiKwD0DsjxEk0NObEWWXKsQkbLt4GelBGGcj6Q58lDMazIfRBW9ZgHJRkHXiDHAw0oEUnZQI6lKClxFgBgaw1S4ngS5ZoEdW7jSNlvACUSxZL7VhsFbp0xElXl62krUSoJ5D6oFOX4HwCILGW/VXlc+q4cw2IrY7ZQevcRW3FOi62Sz3P92+vUY06bOUOsVfvkcesZ7SaS+7Xe/GRZ8rxXUKKgmjKtYi1UJiDHVSK9AKxY8bpY2+0Dc8RaosTQuNBjcxwlRqtUlceQbcv3EBz5mNU685MHeexpfaulAyVJnXgppb2J8p5BY49PIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBXG+IwDTUqkR0WJcqiTEIFsoxIHosTfaLsNlZgeALCVCBdPib8pVuTzzGXlKBXH1TshNnKDLaUP8g15sVYtyhEaWaWtAFCJ5FiKjBILs2HTJrFmNbWox7SVK1qK5VgYx5HbE0VyBItSAgDESr+XlMimlkb5mkS2PjC1PjBKzJGfkSNsiiU5EmW7SRPV9vT09Ig1V+l3o4zn9gmd6jHXvbVerDVOaBJrVWXMBo4Sc+TqzyeMEn/T0jhBrBWVKKMklgdfJq/HCs2eLccc9fb2i7WwosRA5eQ5DwDCUB5DWSVmzBj5HtLik6pupLbHJHL/lStyLFw+kGPhqlq2GwDXk+8x2+USZTzjE0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFfyM/DkRVJVohkeNH6qT4wLXkeIRQ2dixtRwW/aiOrRzTyHEXOSWyIlJiRKw6neDZvlxU4iUsJWLEycj7LFflGBoACFw59qRQluNJJk+WY2E2bOhVj9nYKLfXsuQpIFHibZJYHrNJIkeMAIDny2PEV+JAkMjtsaFHtFSVNmn3STVWxojy8bu3d7PaHmj3pjIOLE8e8GVLjoQBACeQr7UWreQpUTRFpa1ZT7mWAGIjt7dq5PEVKlEzOV9ua1TV86VcX5lMlGvd3Nos1or9A+oxg0Aet1pUj5KWhiSR+85TosIAILTkY2Yiee6qxFpb9Uk6qsp1R9kvjT0+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMO2Eomw3sJMYiVCIRIicnQ4lJg9EgGEyktVnbrBfJ+N3fJUSFtTU1qexLIMSyREuMTOErkiXKKlhLPAgClinwuWsKGDXm/DTk5XgMAlEQi2EpMVCWUY1aMEqViWXpcihYPVC6XxFqgRLQkRj9mVYmC8pT9lityLJOtxKwYZfxs2VaJ51IGgq1EbNlKxA8ABPlGeVtl02qoRSDJG7p1nk90lQtirSmrRAD5yr1pyYPdNvq9GZblMZLNye0Z6O8Xa56jxIgBqCrzpaOMoVCLGTNyLVGikwCgOS+f59sDXfJ2ExrEWhQp7zUAHEe+LnzCNb7x+hARERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKsyBHAdCJaMOSs5arGTxAYCtZCB6SsZYouSIWXU+cmS8rFjrLRXFmhPJ56LlHDq2PoRdRz6XkpJz6CVyNpnWd0o0IADADuQOtIxc6+rpEWv5jNznAFAqy9mTjQ1y3yZluQ/Wbdwg1qZtN1FtT1dBznr0PU+s9Q0MiLWGjJxDBwA5Xz7PvoI8LttbmpXt+sSarQUrArCVSEKjTAdV5T6xK0rgJwAnq4z3jNw/Wq5npOQu9inXGQCyyjUJlexJW7n/LKXfjTIfAkBW6YNICX+1lIsZJ/qE4LjKfKBkvyKRB4nlyH3QWOc+Wf3KSrE2edZ0sVYsy1mYsPQ5OizJOZGx+16SkOnvjU8giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFcb4jANBVo4uKRfl6A0tXgPQIyISJQ5ES56wbDlyAQCqSryE58qxFMZR4iyUBoWR3D8A4Dm+WGsJ5EiLzT3dYq0pn5Pbo8R9AICXCeRtI7lv3UC+VQslPS4liuX9hhX5mpQqFbE2ZdIksdbdI8fbAEDgy9ekTzmXwJbvE9j6vRAn8jjpmNAk1sJIvp75rBz70tOrxJoAmNDaLtZKoRJ3pSS7VOrEelkFuQ+MErUSJ/I4MJ7coMDV316SSIk6UkphLJ9H4Mr3ZgL93qzG8txlQ+7bTFaO0YrqzE/aeSZKJlg21yjW4kiO7SookVUA0KCMaT+Ra4VEvufdOuPS8eQ5sRrpcwmNLT6BJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVBjjMw4kcSLXbDnnwTZKBgQAY+TPB0a58lr8T6xEwgCAUaKFLKW5nitHu2hxFrGeEAEnlKM5IkuOIPFzcjRHrJyIUfoOAKolJebIl9vjKtE3AwNd6jHzgRy/UazK11OLeuru7hFrk7abrLanu2uzWGvJTxBrxXKvWHNc/V4oluU4lUwk922kxDJFSrzUpNZWtT3rN8sxUdm8fL3sRD7PbF6JOQJQHCiItdZcs1jrLSjxUsoco81rAGAp0V3aO5MXa3OFPGaLFfneA4BAuf/iUD4XV5mDLEd/i42V+y8yck2ZDlAuyVE9niVH5gBAvkO+/7r714u1yJb7J470ceDbch+5SsQPjT0+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMOhFU5DsQycgRCLpCjZgCgEsvRJa7y2SGjZPz0KpE6AFCuVuRjKrEdFSV6IlGigRwl5mjLtnI9seS+zTfIUSoVJdLDCvWYo8SRo0K0vnOU+KSWxhb1mFocSKDEwlRsJb7FlcfIpk0b1PZUyvJ52m5ZrDXm82ItqhPRksvk5G2VLKiwqmS02PJ47i7K5wEAubw8DpqVtnotDWLt9VdeU4+ZaZT3218tycd05bY6thwdVAnlfQKAb8nbRpE89jxP3q6rV460mtK+ndqedT3yuG3MyGOvHMtjL1sn1guefP/lHPl6FUsDYs04SiyVPl0CytxmZ+VIHUuLJ6vTB6EyZ3p+nZw2GlN8AklEREREqXABSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKlwAUlEREREqTAHchxIEjl7S8tALIV61pylZCAWlczBgpH36yg5hgCQdeUMsmoiZ0gaKH1glM85Rg82U7oPti0P/6gkt8dAPg9byboEAMtWGqRsm1Ha2tPXpx4zm5UzLcuhlhUqH1PN5nT0a+IHct8GWt5eWc4K1bIBAaBUKIi1xlyLWDOufJ8EyvVyLDkzDwBKZbkP+ox8ngNdm8RaU4OcGwgAvjIO3lz9lljbacfZcnsKclszgXw8AAiVbFPbla9nrGTRtjQ0ibWBip5L2ZiXMzaVOF4ESlujUM/NdQJ5vixG8jzsWfJ2oTZ/e/q4jApyHxktalW5F8JInmMAwAnka1YoybmeNPb4BJKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFJhjM844LhyNI5Rom+iWImEAWA7SvaELUetWFAiWpS4HQAoK9FC+UxWrPWX5EgP35PbY6BHxiRK/yFR+kCJQDJaNBD0mCOTyOcJW962VJG30yKQACBRMkg8JUYkUWJWPE+eOkyoj0s3Kx/T9+VIlCSU+yeuaBkjQKko15vl9BbYsTwOKpE8tlxHjy5xA/mzexTL+81nG8WacfTImMKAHEm06wd2E2sb126Q26NEB5WUiCgAMFV5nCRGbmtDTp5HbE/p10Qfl9q8p8We+Vq0WZ1IqySRx6VnlCijSJlHlOkgVs4DABJXeaYUKf1nKRFtehfAJPI4mTBBjvihsccnkERERESUCheQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKF5BERERElApjfMaBKJJzF2xbvkSVSI/JyNpKJIoSA2H58jG1iBEASJQMCUv5vOK5crQLoEVW6BkRWnCHFtUTKxE2Ro3m0PtHix3KKHEgfUpMTTYnR6kAQLkiRys5yvjSwoGqFbnqKBFRAFDuk6NEkqzymVbJA/EzgXrM9gY5+mXDxvViram5Rax5bkasxRU9LsWz5PEeK/FboRL74uppTgiU+7pvU7dYa+3sFGvLX31FrE2dKG8HAFWlwRmlb10lDmztW2vFWmd7i9qeRIlTC7TxpUSFGatO1JoS+7WpW74mLS1ynFMSanOwHjNm20pkmhbVo8ztRonCAoBKLI/psFJQt6WxxSeQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKF5BERERElAoXkERERESUCmN8xgEvkON2qkociOPpkQwFJR4h6ynRCrESK2Tpxww8JS5lU5dYcxz5s0x7e6tY6xsYUNuT8eT4jaoSn2Qp8RsmUWJWlBgMADBG7r9SUYm3CZUIjTp3sW3J48sYpb3KecaxvF29SB1XGbeOK4+DOJGPGUbyWAcAV4kryuTlMZt1lBgtJbLJ9eU+B/Q4p8QoUVBKZAxsvd/Dqjy+/KYmsbZ54yaxNnXyZLFWKZTU9viBHGXUW5DjW9ySvN+WCc1ibf1a+TwAoL2zQ6yFiRzr5StjFsqYBYAklqPYtp8ySaz19/WKNUuZS02d+Um7r7UYrUSJdzO2Hm3W3Chfs57N8nnS2OMTSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoUxPuNAuVQWa64tRydUtcgFAA2BFmEjR3qYirzf2NEjGapKBEk2kxFrWtJDuSxHXUSRHK8BACUjf0ayLSXuQomwcT15u2JVjy7paG4Xaxt7lb7LybEw9SJsHEeOzdFSmaJQHnu+L9f6C3r0RsaXY3OqVflecKA0VjlHAKgW5T7KZfNibWN/n1hrbWkTa5u69MiYRuWYljK+jBILUwrltgJAQ0aOwyopcVj5BrmtYVUZs0o8EgBUlHHb2NAgb1eRx4hRopUmTJSvFwDERp5LGhvlPhhQxoiTKHFpACqJPLd5oRxzlCjzE4wSw6bdQwCgxEvFynuRo2wHV19m9IbyOIgtfQzR2OITSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhTmQ44CrZNjFkZxrltTJgeyNC2It68qfHSJb3q+a9wWg2C/nyU1sl3PoNhfkXMqGRM5ncyx9CMdVOWPM9uR+1zIioWWwRXp71r69XmmPnPtWVLLSHE/OiAQAO5Svp1Gup6vkt4UVOb8u6+XU9mij1rbk9kSxfC8Y/VYAlAy7opIr2NLSLNY2b5avZUd7h9qcQr98bzpGHpdGuec9V84qBLSEPyBW3gr6eotiLdcoZ7tquYqAPpfEyrX2bXm8x0pOpuUq9y0AVxmZ5ZI8r0WRkrvo6M9oHEc+l03rNoi1fGOjWNPuaaP0KwDYynuRo9ybljJ+KnGdnNqysl+jz200tvgEkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUmGMzzhglHiExChxO4kcfQMAba1yBMnGDZvEmhY9YdWJjMk0NIi1SlmOc3CVGJZQidewlMghAHCVuCLP0WJz5L5tzcn9Wqr0qO3RUoeyGblvi2oUhh5PEiqxQ6Yi94/va30r14IgUNvTV+wXa3k/K9a0GJZQGVuAHk/iKXFOYUUeBxMmtIu1woAc0wMAmZx8niaU428yrhybk2T1e/PVF/8i1nbcZS+xZlXlmKO4JPd7VDfCRu73alXu90CZgwLtni7r18T15b7t6+8Va41ZObYqrJMvZSn3pp+V2+MqUT1V5f3EKDFHgB4b5/ly3ybK/JRvalOP+eLT94q1Xd+/j7otjS0+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMOFPvkeAk/I0cnBBk55gEAerr65KISXaJFRMSRHgNhK5EyZaUWVpWoGSXKKFEiKwAgb8v9VzVyXEpO6ffugc1izbb1KBWj3HKhEt8SVeSYDC8vtxUAEkuJ+VG6L4rk7RxXPmZJiWABgOa8HINUiZTzVCJa+itF9Zi5rPJZOVJiT5TP2FFFjjyxXH0chIl8nrYSPVVO5GM6Rf1e2Gk3ORKl1LNerAVBXqzFyj3dkpWvMwCsfPMNsdbWrowRJWKrXKyKtcSS+26LklhpVOLJXE++p61QP2ahKEckVZX3hTAjj59sTo7R8pToJACIlVghxPK5REqsV3WgRz3mnD0OEmubNspxczT2+ASSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhSYYzPOOAoMRAVJdrFMUrkAoBIia3QQkZsS456iOrE5sCR60msxPE4cgyE58rtKSnxNgBQsuWYDNuRe2GgKu83DOV+96HHdjiBfC4J5Gvd1NAi1kpVOX4E0GNhYiWqJxPIsTnlqhyX4tWJsCmV5GviZeVoqmpFPs/mlib1mGEoX09jyf0TxvJ5+sp9i1CPu7JsedtqWW6r6ymRQ3Xuzbgix8K4uaxYs5TooMCXz6O30KO2Z2Jri1xUoruCQB5fhUSO+Ak8Od6mnnJR3m8mJ28XVuTxAwCuLV+zpu0nibVKRb6Hgox8nm+8+bbanimT2sVaqET8mEQ+D6dOdFAlksdXY1uLui2NLT6BJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUmAM5Dvh5JZ+sLOePxbGeNec78uWNYjl7q6pl5umRXmh25FC03kJRrPlKnlxZycJUYtQAAI4nZxmaRO4/Y8k1LcPPcvUGVZRcOC13MYzlPojrZE9aofw50VEyNo2S+5Zx5ba6Sg0AKrG8X0vJkzNaPqmyTwCwtMxG5T7K+HmxFipZfFadTEajZCu6yjWB0geWrc8HhZ5+sdbY0iLWSmU5P9KCkrNaZxzkMnLflo0870EZl54yj1QjPTPWs+X7xMvK2ZMx5H53PD0T1bLk9paqct/mGuXcznJBzkud2N6qtse25BxWG/I1sV3lvSbR+91K5H6vRso4oDHHJ5BERERElAoXkERERESUCheQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKY3zGgf6eAbHmB/Ilso0eFRJW5cgY15OjgypKXINv6zk+vd1y5IfryJ9XokSOqfEsebs40iNsbMh9ECtRK5ESs2JB6Xej31JGSVpJjHye5bIcgaTF/wBADDn2xFVqthIZEyixHXGdaCWjXE9LSePR2oM6sTlK0goM5JiRXEaOS5FDcYCoIkepAIDjyPEuiRLZZIw8Lj1biQMD0NjYLNYspYM8X+4DY8v3V07ZDgAGBuS5IpuT48BCJRZGScWBU+fedB15DIWhEh2kRETVi6HJZORx4FblWrmgRZvJ46BQld9rAMDy5OvpqFE98vjxlfcaAOq96blyrBCNPT6BJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVBjjMw4ESgwElJSagZIcuQAADVk59iQK5W0DJWYlCuX4CADI5uTYhagq5zUEvtzWYlGO+/CUaAkAyOfkKJFqJMd2FKpybE4cyZEeJaPHdnieHM1RDctizVeiemxXj7BJlKijOJH73VX2WyjLMTWWrV+Thgb5mpTLch+4SvRNqMTbAICllI2R99vX1yfWYksez06dPrAhHzMySqSVo+y3TpRRFCudoJyLrxyzVJGPWQz1e6FYkseQdp+EWnRXKNeMEpMFAFDmyyAjz2tdXevEWktTq3rIalWJC1Mup6Xct9r4cbT3GgCR0h7Hl7c1ShyYZSs5PQBg5HOJQzmyicYen0ASERERUSpcQBIRERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKlxAEhEREVEqjPEZBxJLjkCoFOQojNaOZnW/PV29Yi3w5VgYGLk95YoeHeQokR/ZrBzNUY7k6AktwiZUtgOAgYoSC6NErVhGiRxy5dgJ2wvU9lQqcixFLHc7PFvO9KhW9T7QYod8JZojUaI5XF++libWYzuMEhmTKJ2QycjHtGP5mgBACco4UK5n2C9fL9eRo11sR++DYmVArOUyObFmlBitRMv8AqAlC5lEbm+cyNekIZdXDqg/n/Cy8r0SKnFOcJRrrdxDtnKOAGC78vjq7+0WaxNaJom1ONKjjIzyDEeNxlHeMyyle3xHmfcBQOmjKJbHnu0p91CdfneU8eV4fMY1nvHqEBEREVEqXEASERERUSpcQBIRERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKozxGQdK/UWlKq/xB7oL6n4dJe4iiuQ4HjuRh4WjZUQASBI5SiSqynENWvSLq0TjRIkcs7KFHH/TXyrJx1S2KyvnmFFijAAgUSKScrmsWAuVKB7b0j8HGuWauZ4SZaTs01OilaI6UU/anrP5RrFWGOgTa45yHgDgKX0UVeVxEOTkSJ0klseBFsm0pT3yNYljOfolUCJ+SpV+9Zi+pYwvZVw6Rr5eTiTff/lmJeIHQKlPjupxlPEFpT1xKF8TLT4KAArdcuxZS0e7WOteu0GsZZoa1GPaSlSPbctjxCixXpYtn6dVZ64IYyW2SrnFqsqc6CttBYAkkuthtd78TmOJTyCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBXmQI4DfpARa9mcXFuzRs4fA4CpO0wUa+WynK+l1XJ5JZ8NQKCEhRUG5P16GXm/WkZdXI3U9mjVrC/3baIE+RlHzlmLQj23TMtsrFblLDUlLq7ux0At6zFS8iX9jJy/mShZfHA9tT2RktXnBXL2nePLYyTj6ccsV+RsRSXCDpat5EcaeXS5ynYAECtlo0zL/QM9Yq2psVU9ZqhkNmrn6SrXM47ke6FnU7faHs+V779IySMMfCUL01KyOSHnTgKAl5HPMzHyfu2cMifqEYiwlIxEbQ6ybC2/Vb6WYUm+DwCgs6VNrL3du1HeLj9BrK3oeVs95lRl29jW329obPEJJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIYn3HA9eTokv6BkljbbqIcuQAA5VJVrA0MyLXIyBEapk4uRWTJ52J7SiwM5JiMjBIjYmXkSA8AMMq5xErkTlWNmpGjJer1j6XccXGkHNOX+8DESg4NACUFCbYrf4Y0iRzRAiNfZy1GBAAcR+6jsCRH41iWvF2kZfEAiJUYFt+Xx2UUy+3xHPma2PXaA7lvTSIfM59vFGulUr96zEpZblNDo7zfSkWOv/GU2C7tfgeARMmmSpSxVyj1KceUx6UPPerJVqKM1ix/Xax1bD9FrGnXEgBs5V5JlMlCu+dLynUOAr0PitUBsZbN5sXa+v41Ym1iRo+Xqij3SrWsRy/R2OITSCIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoUxPuNAGMpRD64SC1MoyBE/AGDZcqRFksjbtrW2i7VyIsf/AECoxA5lXTmuIZeTIyJWv/GmWJvcMUltT6mqRL8o0UGJLUeMVKsVsWb7csQPACBUokuM3NZEiQOxHf1zoJXI+/U8eQpwlNnBdpSInzqfS5WuhaXEsERV+XpZSjQQAPiuHCljErlBxlKilTz5mNWq1j8AlGsNS+6/qKrEHCn3OwBM3XF7sda1frNYi5VInUCJI0os5UIDSJT4FqPFHCn3rZPIgzasyrFdAJBTIpJaJsmRaYEr93uprM+XRhlDSSy3N1L6p6lJnkujWB+XfcWCWMv6GbFm2XJNSQoDAPQrUVnNTU36xjSm+ASSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhSYYzPOGCUtItYiV3QIk8AwFaiTVqycixFYaBXrGXyckQEAHjNchQGQjmWolQsi7VJnXKsEJSIGgBoaWgQaz39cmRFYHvyTm3tmuifyUpKDEsmI0dhaFEqdRJs4Lrybe568nlayjGrJTnKyM/K5wEAym5RjeRxkGuUr2VJiY8CAMeWO8lYSmSMErcTx/KNW29itQI5ViiO5JgaKNEuRon/AYCBvm6xps0kWV8eI1oEWSar90LiyPuNlLnCUrZLIuXeVMYAANhKc1vbWsVab3efWHOVtgJAGMoxP4lyVVztmihxRXWmS7ieEkNm5PHlKSM+VOKaACCjxCD19sp9S2OPTyCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBXmQI4DRssnS+ScNctWMrsAGCUTrRQWxZrtKBl1FTljDADyTXJW34aefrHW0dYi1oolua31st0qSl6hreTmqVl8tpxb5imZngAwoSEr1qrKfqsFuQ/cQM9d1LJETUXOoWtQrmV5s9Iev14OpNxJxshtLQ4MiLWGppx6zLAiH7O7R86ay2fk62UrbYWlj0sLcj1ScvM8V743PVcffJWSfK1dR74XtGzAipHbajt1ciCVcWk78r0QKse0INf8OvdJokxt/coc1NMv5+a2T5DzdrccVB4HfiBfk0C5JgPKXKG8nQAALGUMGVfe2LGUa10nn9RWslb9oE5wJY0pPoEkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUGOMzDiSJHGfhenKchZIEsqXsKTEjxpNroRzlEDn6QXs2y5EobiAPt1JZjogwyonWSUuBieX+czx5YzeQ+yeJlCwMpcsBYFO33D+5pmaxpsWaWEosDgA4SpyK58m1KJQjUTI5OeLHUdoKAEYZXxlXjicplktirdAr1wAgVjJa2pqbxFq5WJZ3qsT4WEq/AkCpVBBrjiOPvTiWx14c6ePAy8gRQIWCHHfV0iLH3+TyebEWhvI+AcBK5HGizYlJJN+3nhJHVA71CLKM/+6iaCZO2k6sFZToKQDIBvI1iZT7uqTcC0FGPg9Xub8AoK9fbq+tRPXEthLnpEQVAYByOYE6MW00tvgEkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUmGMzzjgKVEFiZYLo6d2wFViKaKqHLsQK/ERNvSIliCXFWuOI5+L7cjxEqGSmhMn8nkAgG3L7Y2VOB5X2c5RojAqidJYAPlsTi4mcr/7yhhxbP1zoBbvYrtK/IbStbYrj61KRY/UcZWIm0SJ+HFtuRYEetxHGCrjVhnvlitvF1Xla22FVbU9mYwcjZPESoQN5IviKDErWzaW29vUJMfxvL7iNbG2w+wZYq3cp4+Dzo4Osba5r1+s5bV5TZkvbej3ZpzI18x25XssjOTtgrw8HwJQY78CJYIs1yBHfhViud8r/b1qc4JAiflR3m/iWB6XVp28uYoy3mNLv2Y0tvgEkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUmGMzzhQrsoxEJ4SJ2N5+vo/iUK5lijRQcpu46SiHrMhK8dWhEp7MlpMRkUepq6rR0QkRo6XMIkcHxEpET9K2g4SZTsAcH05viV2lfiWSMn70HKOAHhuIO9Xid+AI4+9sFwWa7ajj8tAiQCKlDECW96uqkSBAEBiyW2Ki3L/VUN5vOdycuRJtc410T67x0peihaipdzSW7ZV+qBSlaNfps7YQaw1efJ43lTsUtvzxltrxFo+J8cK9VfksdeYlce6ZekRZJYShxUr19PLyOMgMfq47N7ULdYaGxrEWpRsFGuxkg1Ururzd97zxJpR9qvNiaWSHufkK9FvXp25hMYWrw4RERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCGJ9xIJuXYwySUI5ZsX09lqJckCNRLCV2wQ/kKAct2gUAStWivF9b3u+b6zaItUmNLWJNCX0BANhKtonryPEbcSJHKzlK1EW+UY4fAYA3Vq0Xa5OnTZbbYynjQIl6AoAkliNIHOVa+0o8iTFyv1aqdWKFlGiTUlW+osaRj+lbelyKb+Q+qtjyMb2MPEWWi3I8iZuR420AwLLk/TquErUSy33gOfp0HlaUMa3c11EoTxZdVTmGpq2zUW2PYylRYokSm2PJ82VvX49Ya2meoLZHi6mxM3L/JEqGjTbHAECQk8eJlsoUK/O3lhykRQ4BQEWJ0Qo8eVsnIz+LCrycesxqVW6w79XJpqIxxSeQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKcyDHgYHefrE2oblZrHX3ydsBQD6Q89KqkZwJByVTL1QyuwDAD+S8tGokZ7u1t7aJtUJJztvz7DpDWAm81LLmsn4g1oqQtysPlNXmNDc1iTVLyenr65LzNVuntKrHLJXk/QZG7p9YyX2zbPmzp+Pq2W2xMvZsT8nbi5XUz0QfB7FyraNE3m/eke+hfi2b08g1AKiW5KzHjJLV19wizwcvvLpCPebkzoliLVSuiZYfaStphZmsnv8HJb801PpPCUHMZeX7q1ysk08ayGMoUeaulrycL/nGm6vVY06etoNY6+6RMzZdI7c1MvL1MkZ/ZmQr9Yql3ENK39bLK3YDebwPlHrUbWls8QkkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwhifccC25CiMKJbjLFwlBgMA4kSOc3CVuJ1yVd5uQpMeGVMoKvEkvnyelYoc1ZPNyZE65bIem+O7cuyQo0T1VEvyfl1X7jsr0a9JrkH+zGa5cq29LS/W4qoeT+IrMRnaGIpCOd5moFCQj5eV+xUAkkS+JpYSIWVZcv8kod4HkZH3m7XkaTBUImMashn5gErkCQAksdzvcSKf50Yl2mVKe7t6zFxGvi49Jbl/Ghvk7RLleplQj/yqlOW5Ako8kBa3UynLcVcTGuUIJADoV7Z1LPl6rd20Rqw1NTTqx+zrFWvKNAMk8rh0bU+sGeV6AWqCGxwlnauhvUWsPf/MMvWY79/9o3KxTuwQjS1eHSIiIiJKhQtIIiIiIkqFC0giIiIiSoULSCIiIiJKhQtIIiIiIkqFC0giIiIiSoUxPuNAQ0NOrIWJHAfiufrl02JPLEfOawgcJQYi1uNJmpvlc+nr65ePqUYSyZEVTp0R7CtRK91dA2KtIS9vt3mDHGHTosTtAICjRMYkSuyJrfRPUpXjdgDA9pTraeS+jWJljPhyNJCp6lEhoS2PITeRY6ucQD4P2ygZIwAQyefZV5UjpHJBVqzFSsSWo0QnAUDkyDFReSViC6ESIeXozwNKSixTYyBH9RglMsbY8jGVNBkAgBfJ94Lny/3+2qoXxdrUSdPEWjmW48m2HFNucLUs910+3yTWklDuOwCIK/I4CJS5S4tPcpR7M6rTB9ozpWpVjl2q9PSJtdm7flg9YlyQ+8Ax+vsNjS0+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMOVMtKVIGtRNgE+uWLIUc9uMm7i4FYs2atesxpM6bK+3XkqJVAiSSqyGkpcKBnhfQV5OiJxkY5cseCHJfSsZ0ceWLX+UwWKlEqRonmsJS8onoJNkrSCiwlZUSLgQqNHAcSKXEfANAysU2sbVizXqw1okGs2Rm93xOl7Lly0fXkcVAqKfEjdcZlzpHHUEWJc3KVi6mNHwBqJ0SRfJNFsTxIsll5n3GiD0xL6Xco81N72xSx5llyv1di+XoBgGMr81NOnhNDZRzkm1vUY3ZvkKPErFC+JrZyqYuFHrHW1CxHDgFAoShHWllK/1hGiTkK5PsWANYXimLNaO+NNOb4BJKIiIiIUuECkoiIiIhS4QKSiIiIiFLhApKIiIiIUuECkoiIiIhS4QKSiIiIiFJhjM84YHtyPEJVSZ4wlh5xEGTkqJCBPnnHliXHR0yZPEk9ZqkkR7jESjRHMZT7wA3kWhbyOQJAd9dmsdaUyYm13rIcZ9GQyYi10NSJndAidZSip0XqKJFDAAAj97uB3LdOIo+DjJ8VawNyKgcAoNDdL9byefma+EqETSSniAAAQiVayPLk/VYr8nYZT452yQX6uHx1/SqxNq1TjqkZKBXk9gRyewA9rshRYrRsJVYojpSacr8DgOfL43aDct92Tpgg1iqhHC+VCfRoJRPL94JRsrJyTXI0Tn+vHNMDAE1tcqSVrdzzfV29Yq1jkjx+ejbJMVkA4LryNbGU+8915fG+ccMG9ZgNjc3yMTN1MspoTPEJJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwhzIcSCTlTO0kkTOocvWyTXrVgL58lllW1vO3qpU9cA9x5NzxHJZOT+xv09uayaXF2s9/T1qe0wk5zLGyscnLQ8trMr7tB39M5lRykbJXRwoyf3jefptbJQcSNeRtw1jJcevKOdk5pr0cekY+Tx7++XzTIJErHnKeQCAr/RR4MrtrSh94CgXsz+S71sAmNo2WawNlOWM1pwvzxWmzvOAWMsDVTIbo1jOVrQtuV+NJV+vLfuVaxOaG8RaVbmnHUe+byPtgAA8Tx4HvQU5uzRrKXmgvpxrCgC2ksMaK+di5+VxsL57o1hrUHJWAaCq5N9mbPlaR5DHT3OTnPMIALFyH9m2/J5BY49PIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBXG+IwD/X1yRETGyBERFVePpcgrERKFshIPlHn3w8LS4kCqcqxHNie31YTyeQZZOeIHAHKZRrFWLcnxJBkl0gO2fI6+q8dOlGM5BklJRAEcue8SJZ4FgBIUApSrch9o5+l68mfPckWPsLF9eVs/kMe7BSUWRolAAgBLiUQpV+X2ukpsTrEob5fzs2p7qqEcRZPJyH1gy5tpvbNlWyWGpRwqUT2x3LdabJev3UMAqpF8L2iRXz398nzpJsrYyujtCSM5PqmxQb6eYST3fJDVn9F0Dcjn0qTEl/nK/ec5ct8ZowwgANm8fMxC74BYyzXK24VGH5meMobiUI+No7HFJ5BERERElAoXkERERESUCheQRERERJQKF5BERERElAoXkERERESUCheQRERERJQKY3zGgZwSYbNhXZdYm9G2g7rfshK/kURyRIuJ5Zpja6EwgOPKsSdxosRdWHLERimS41ICR2+PpUXR+HJcihZhU63I0RJJUifCxlIidyz585zjyf1aqpTUYyaxHN1hKdEuriv3bTVU+rXOrBIrcU7FPvlcWlvlSKZQicUBAGMpfeDJ46CsRPU4gXyiWkwPoMddabFMnnJ/9RT1cZCz5LiUihJp1dEg93shkucYW7nOAOC4cntsJY7Hc+TrlVHimqI6UU8ZW56Hi2X5ng98JR5JmfMAIO/K8UADyjXRIpJcZYqJlTkGACpKlFFTILe1pIwDJ9CP6dhKjE+d2CEaW3wCSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKlwAUlEREREqTDGZzxwMmKpva1NrK1atUHdrYEcIdHa3iTWEiV6wtSLaInl6AnXl2MgIiU6yFU+51haLA6AWNkvHPlkEsixFH5Wvl6l3gG1PX5GPmao9HvGlmueEiMCAEbpv6Qs909FiaLxlBgRO9GjlYrlolibuP12Yq130yax5ufkeBtAH0NhKEe0eFk5MkZJ24Ht6J/NtTQsbRwUjBwrlBg9psb15TY1uA1irRjJ/QNLialx5TECAEkkb1tJ5HnEMXLnqfe7Elm15ZjyeLcd5TyVGJo6qWeoWFrfKtFTkHesXa5E2ScANOTlKKPV63vE2vZKxFZFifgBgNCS655VpwNpTPEJJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwhzIcSCTkbO3+ktlsdY2Sc5uA4BAyT3rL8k5a9WynBXmRHruoq3kAwY5OS+tVJSzAZsb5NzFcqTnmsWxXDdKtBuUDM0gL29l+fI5AkCUaDl1cuZZpSpfL1/JoQMAN5CzDGNP3m9gyeNnQBkjE3L6tLK6X77Wkzrl8V5W8hqbLDljFAB6Ivk8PSVX0CiZjLDla2m0PEIAoRIi6Sn5iVXlPNqbm9VjFqpy3/qufM2qrtw/dizXkkTP/7M95fmF0n2W0p5Qud8HSiW1Pc05OcvQVp61lBM5m9My+jOanC/fm8WKPPYSpYO8jDx+wjq5uZWSfC4zOuVM4rKynW3VeU6ltUm5x2js8QkkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwhifcSCqyNEcWsSBY8vxNgAQKSkanhLb4TXIsTAJ9MiYqhKVEZWVSKL2VrHW39cl1nxPjsEAAMfIdS1hIwzlfo9LcpxMHCrXEkBDVrlmyrmEVbk9mUCPc3pr7XqxNrFTjn4plJXoICWuqBLq8S2zp7bLx6zI27a0TJC3M/I1AYCsLUdlaSkj1VCOhQnktBTYnhw1AwCBLV/r3s0DYs3NBWKtWJG3A/R7N6zI96bty3OFbSnjINKvia90vIHcf65SsyFflCZPze1CQyBHQfUW5b7NuvL9V6jIkVUAECby+EqUyK+cK/e7Np6zGXn8AMCKN9eJte0myzu2HXmMxNDng0AZX9Biz2jM8QkkEREREaXCBSQRERERpcIFJBERERGlwgUkEREREaXCBSQRERERpcIFJBERERGlwhifccAYOcohm5OjJapVPSbDVj4fmESuhWFFrDmBHk/S3CTHwni+HMmgHdNSokLiRI/mcDw51qMayf2uHdPEcrxNxtNjMrRL5kDeb2NOjqFZvnK1esxZkyeKNaNkfnhKNIf2yVPbJwCEJTnWI1FSmfxEHnt1bgUEWXkcrFr/hljbcdL2Ym2gKl8v264Td6VEHQUZud8b8/L46urtU4+ZC+RtLVvuHy0hKTJKzJir5BwBcCz5elaUC+ra2tuW3B7P0a+JFrnTmpGjenoqBXm7rHzfAkCpIB8z78vXqxQqF0W5JijrfbBduzx/Zzz5XCIljijU3zIQKfOw3loaa3wCSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKlwAUlEREREqTDGZxzQUhdKxbJY89Q4C8By5RAE15NjRFxbzlIxnv6ZI4a836gkR+7Yibyd5ctxIEkiR6kAgBryY+SqUS5KZORcCkuJEQGASIlvydoZsbZx/WaxNnVyh3rMghI347jyudhKiIZ2Hpal94EWteLG8jEHSkq8VEaPjBko9Iq1yR2dYq27JEe05JR7YWO/HM8CAG15+VoHeTm6q79YEmtWneigsjYOIEepZJT22Eo0jrzHLUpleW4D5OsZKVOQFStRYXriFzKuPJ/2V+R+d5XIr4ql50tVlXPxyvL1cpWxZznyPe3WeWa0uU8e780drWKt1C/fmy2BPNYBoFyVr5kx8jxDY49PIImIiIgoFS4giYiIiCgVLiCJiIiIKBUuIImIiIgoFS4giYiIiCgVLiCJiIiIKBXG+IwDlrKMd1y56NSJ7YiVaByjRNFo8TZ2nYgWX4nCyDTKcQ4b1mwSaxOa5RiRcqVOWIhyLo7S8WEsx1IoyTcwdWJ88kp8y0CXHDXT3N4s1uJYj7pwHPmaWJYS46NcaxPL2zlKtAsAJIl8zaqRfC6BL5+Ho9QAIFKaFMdyexxf3jBULnVTLlDbo42SSkmOjPGVaC7LqROfpAwTNQLIku8hy1YimRLlRgFQjuRtfSVmrDkrzwfdSlxTNqNfE+0+ch052ixK5DiiSE8ZQ4PyFlxWui9w5ZijCuTooEjLjAOQb8jJ+x2Q58RAmWOqkT5H+8p8UdVuMhpzfAJJRERERKlwAUlEREREqXABSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKkwB3IcKA7IuW/5Rjm7rFyR89kAwPXkfC1b+ewQJ0rumxZaCSBUchndQN7vhJYWsVatyPljNvQ+iCGHqUWhvK1rKXl7Sr5dNSyo7cll5euZaZSz5oySZ1lPrJynF8hTQKSNA6WWCfQxUipomYNKdqmr5FIq2ZIA4GrjQMnG07YrVeX8vwblOgNAFCnXM5b7L1K61tTJXVTzQJW4vaQst7Uay5mD2nkAgCnL12y3ee8Xa3/7y4tiTcvCTGL9HrK0+dIoGZGW3K8tjXKuIgA8u+JJsbbblA+ItVIin6enze11YnMDTx63Vcgba8esVvWcWtjytlrOKI09PoEkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUGOMzDuSbm8VaXJVjMiKlBgD5nBwLU67I0UFIPKWmRzIksRwlks/IkRZ91X6xZlWV4ykxKwDguPIQLxs5HshVol0a8/J5vPL4S2p75u4rR3P0R/K5WJD7vVzRx4ESmoOqkt/i2XLfOZ683ca1m9T2NLdMEGtaTJQJ5WPGWg4NgFjpI0fpoCiSt9OilUoFeWwBQGjkSJR8kJWPqcQnoU6MTwy5jxxLiTnSjqnss7kzr7Zn0+oNYm31mtfEWqZRHpehcp2TWJ+7TCSfS6ahSaxpc2nPxi71mNOb54i17n4lvsyR7xNXibsKtfgoALYSX2Yr+y0rUWuWrc1AemycW2dbGlt8AklEREREqXABSURERESpcAFJRERERKlwAUlEREREqXABSURERESpcAFJRERERKkwxmccCDJyVEEhlDNs8o2But/CQFEuKokfRomMsSw9BiLT2CDW1q19S96uSY4y0tJJolBvT6xEbFi+3O9GifHZ3Ncr1nY/QI7pAYCuTXKsR0O+UayVIzn2xXeV2CUARomMAeT+czwl2kWJVso1tajt0WJhHFv+TKukrACJHmXkZZSpThlfsRLVk7Hk+y9O9FghVJSolVC+/7R7IawT65VrkOOBImUcQIn4sZTYl7VvyjE9ANDeKo/3sKjMQUrXGleJodHymgAkyvOUUmVArIVKPFAmo8/R/eU+seb58vUKPPme71PmfVeJ/wEAJ5D7KAzlecTS9qvc0wC0JCjEynsRjT0+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMOFHr7xZqvxECUqlo8C+BnfLGWRHI8QqhEMti2HhljOUoEiRJ3gZKcC2NBiY9w63wGsuRoEz+Q+7ZSLos1N5Bvm0KvvB0A2EFOrPWXlPiNRO73WDlHQE2pganItVJSEGtBVj4PW8uaAWDbclTIxt71Yq29oV2sVS19KotieQxp0SaBp9x/Rfla10suyQTKvamM6UQZl7mcHhljlKgeW3mWoEVamVi+b1tbJ6jtiZXx7jny+HKU8eMpsUtaXAwAGC0iSRk/viePvciqM0d7ebGWKP1eVCKbsll5bEVKHBgAlCvyhOAo+UmOlxFrWiwcALhGbm+cKBMUjTk+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVJgDOQ7kmxvEWliQc9Y8V798sZJdFlXk/UahnPfV2NmqHnP9unViraOxWaxVKiWxFgRyBqKlZJMBAGK5nih9q+W+ObYcGGd5ek5mWJaP6bhyvl2i5Mm5nv450Ci5jKGR9xv4cj5btSxfLzer5xGGZTnDrjHbJNbKVTkTzlKuCQDEiZxFZykBgVGkZD0ql7resDTKMeNQGSNKZmWkZLsCen5iZORr4irbwVLGnq13gmXJ+zVKZmxFyTLMZOQ8wpIyfgDA95ULauTzjCtyW92sPkcbS942jpWsR2WeCRMl71MLuwQAS6432kqGrXJPW7aSzQnACuRj2kq/09jj1SEiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolQY4zMOLP/bcrE2ecr2Yq2UyNEJABAoMT/5NjkuZcr2U8TaY8ueUY/5vtk7iLW+/oJYs5SRWAnlqIsIekREovSR68gxIw1KdFAUyse0lNggAHACORqn3N8v1jxXjsapl8xRrspRNPkGOfbEGPk8o5IcpRIr/QMAUVXuo1yDfJ6xcr0spa0A4HlKRJJyzeJQiSdR4pOSSG+Pr8SwVJX+85SYlViJBgKAuFIUa44rt8dtkmtavxrlegFAX6FHrDX5jWJNSfFBWJKPGVbleCQA0JKgYiVWyLHle7rUJ8ddAYCXkbfVnu9Uq8q41DKk6jwyUhKkUMrK+y1We8Va7GTVYzYo7c3m5PmJxh6fQBIRERFRKlxAEhEREVEqXEASERERUSpcQBIRERFRKlxAEhEREVEqXEASERERUSqWMUbPWiAiIiIiegc+gSQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolS4gCQiIiKiVLiAJCIiIqJUuIAkIiIiolT+P/dwOyJU/6V6AAAAAElFTkSuQmCC"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH5lBTWe-rUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}