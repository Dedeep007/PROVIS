# PROVIS: A Text-to-Image Diffusion Model

PROVIS (Progressive Vision-to-Image Synthesis) is a state-of-the-art text-to-image diffusion model designed for high-fidelity image synthesis from natural language prompts. It integrates a CLIP-based text encoder and a UNet-based denoising network, trained using a DDPM-style scheduler on 64x64 images.

---

## ğŸ”§ Model Architecture

### Text Encoder
- **Backbone:** `CLIPTextModel`
- **Token Embedding:** `(49408, 512)`
- **Position Embedding:** `(77, 512)`
- **Transformer Depth:** Multiple `BasicTransformerBlock`s with self and cross-attention layers.

### UNet Backbone
- **Downsampling:** Hierarchical convolutional and transformer-based blocks.
- **Bottleneck:** `UNetMidBlock2DCrossAttn` with attention and feedforward layers.
- **Upsampling:** Multi-stage `UpBlock2D` and `CrossAttnUpBlock2D` with resnet-style residual blocks.

### Key Modules
- `ResnetBlock2D`
- `Attention` (self and cross-attention)
- `GEGLU` feedforward networks
- `GroupNorm`, `LayerNorm`, and `SiLU` activations

### Parameters
- **Total Parameters:** `1,063,773,419`
- **Trainable Parameters:** `1,063,773,419`

---

## ğŸ§  Training Setup
- **Scheduler:** Denoising Diffusion Probabilistic Model (DDPM)
- **Image Size:** 64x64
- **Loss Function:** Mean Squared Error (MSE) between predicted and target noise
- **Tokenization:** CLIP Tokenizer

---

## ğŸ“¦ Installation & Usage

```bash
# Clone the repository
git clone https://github.com/yourusername/PROVIS.git
cd PROVIS

# Install dependencies
pip install -r requirements.txt
```

### Inference Example
```python
from provis import PROVISModel
model = PROVISModel.from_pretrained('path/to/checkpoint')
image = model.generate("a fantasy landscape with glowing trees")
image.save("output.png")
```

---

## ğŸ§© Features
- CLIP-powered text conditioning
- Transformer-augmented UNet backbone
- Multi-scale attention and residual learning
- High-capacity architecture (1B+ parameters)

---

## ğŸ“ Directory Structure
```
PROVIS/
â”œâ”€â”€ provis/
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ scheduler.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ configs/
â”œâ”€â”€ checkpoints/
â””â”€â”€ README.md
```

---

## ğŸ§ª Evaluation
- Coming soon: FID, IS, and human preference benchmarks on MS-COCO and LAION datasets.

---

## ğŸ“ License
[MIT License](LICENSE)

---

## ğŸ™Œ Acknowledgements
This project builds on open-source contributions from OpenAI's CLIP, Hugging Face Diffusers, and the academic community working on diffusion models.

---

## ğŸš€ Contributing
Pull requests and feature suggestions are welcome! Please check out the [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“« Contact
For questions or collaborations, reach out via GitHub issues or email at `your.email@example.com`

